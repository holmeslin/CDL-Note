# 4. Modernize Infrastructure and Applications with Google Cloud

## 4.1 Modernizing Infrastructure in the Cloud

### 4.1.1 The benefits of running compute workloads in the cloud 

Why should an organization consider running compute workloads in the Cloud?
Let's explore some benefits that running compute workloads in the Cloud can bring to an organization.
We'll begin with total cost of ownership, or TCO, which is a measure of the total cost of a system or solution over its lifetime.
It includes the cost of the initial purchase, maintenance and operation, along with any other associated costs.
Cloud computing can help businesses save money on IT costs by eliminating the need to purchase and maintain physical infrastructure.
Cloud providers offer a pay-as-you-go model, which means that organizations only pay for the resources used.
They also offer discounts for long term commitments, which can further reduce TCO for businesses that are planning to use Cloud services for a long period.
Next, there is scalability, which refers to the ability to increase or decrease the number of resources such as servers, storage, and bandwidth that are available to a Cloud-based application to meet changing demand.
Scalability is important because it provides a means to meet changing demand without having to make large upfront investments in infrastructure.
If a business experiences a sudden spike in demand, it can easily scale up its Cloud resources to meet the demand.
Conversely, if they experience reduced demand, infrastructure can quickly scale down its Cloud resources to save money.
Another benefit to Cloud computing is reliability.
Cloud providers offer a high degree of reliability and up-time, which gives businesses confidence that their data and applications will be available when they need them.
Cloud providers have many ways to ensure the reliability of their services.
Google Cloud for example has multiple data centers located in different parts of the world.
This helps to ensure that if one data center goes down, the others can continue to operate.
Cloud providers also use various technologies to monitor their services and automatically detect and fix problems.
Next is security.
Cloud computing providers offer a high level of security for data and applications.
Organizations need to be sure that their data is being kept safe.
In addition to physical data center security, Cloud security features include data encryption, identity and access management, network security, virtual private Clouds, and monitoring services that can detect and respond to security threats in real time.
These security features can also help to ensure compliance with government or industry regulations.
Running compute workloads in the Cloud offers a high degree of flexibility for organizations.
Organizations can choose the Cloud services that best meet their needs at any point in time, and then change or adapt those services when necessary.
For example, a business that needs to increase the amount of storage space that it uses can easily add more storage space to its Cloud storage service.
Finally, another benefit of running compute workloads in the Cloud is abstraction.
Abstraction refers to how Cloud providers remove the need for customers to understand the finer details of the infrastructure implementation by providing management of the hardware, software, and certain aspects of security and networking.
For example, a Cloud storage provider might provide a way for customers to store files so that they don't have to worry about the finer details of how the files are stored on the Cloud providers' infrastructure.
Abstraction also lets Cloud providers offer many services.
For example, Google Workspace lets customers run productivity applications so that they don't have to worry about the details of how the applications are actually run or maintained on Google's infrastructure.
Running compute workloads in the Cloud can help organizations get their products and services to market faster by eliminating the need to develop and maintain their own infrastructure.
At the same time, it provides a platform for innovation by providing access to the latest technologies and tools as and when they are released.

### 4.1.2 Virtual machines

Traditionally, various technological pressures compelled many organizations to tightly bind specific computing hardware resources to specific applications.
Virtualization, technology relieved these pressures.
Virtualization is a form of resource optimization that lets multiple systems run on the same hardware.
These systems are called Virtual Machines or VMs.
This means that they share the same pool of processing, storage, and networking resources.
VMs enable organizations to run multiple applications at the same time on a server in a way that is efficient and manageable.
Compute engine is Google Cloud's infrastructure as a service product, that lets users create and run virtual machines on Google infrastructure.
There are no upfront investments, and thousands of virtual CPUs can run on a system that's designed to be fast and to offer consistent performance.
Each virtual machine contains the power and functionality of a full fledged operating system.
This means a virtual machine can be configured much like a physical server by specifying the amount of CPU power and memory needed, the amount and type of storage needed, and the operating system.
A virtual machine instance can be created through the Google Cloud Console, which is a web based tool to manage Google Cloud
projects, resources and Google Cloud CLI command line interface by using infrastructure automation tools such as Terraform or the Compute Engine API. 
An API or application programming interface, is a set of instructions that allows different software programs to communicate with each other.
We'll learn about API's in more detail later in this course.
When you use virtual machines, compute engine bills by the second with a one minute minimum and sustained use discounts start to apply automatically to virtual machines the longer they run, for each VM that runs for more than 25% of a month.
Compute engine automatically applies a discount for every incremental hour of use.
Compute engine also offers committed use discounts.
This means that when committing to use resources for either a one year or three year period, discounts are offered over the on demand prices and then there are preemptable and spot VMs.
Let's say that a workload doesn't require a human to sit and wait for it to finish, such as a batch job analyzing a large dataset.
Costs can be reduced in some cases by up to 90% by choosing preemptable or spot VMs to run the job.
A preemptable or spot VM is different from an ordinary compute engine VM in only one respect.
Compute engine has permission to terminate a VM if its resources are needed elsewhere.
Although savings are possible with preemptable or spot VMs, it needs to be ensured that a job can be stopped and restarted without impact.
Spot VMs differ from preemptible VMs by offering more features.
For example, preemptable VMs can only run for up to 24 hours at a time, but spot VMs don't have a maximum run time.
However, the pricing is currently the same for both.
Finally, Compute Engine lets users choose the machine properties of their instances, like the number of virtual CPUs, the operating system, and the amount of memory by using a set of predefined machine types, or by creating custom machine types.

### 4.1.3 Containers

Infrastructure as a service, or IS, lets users share compute resources with other developers by using virtual machines to virtualize the hardware.
This lets each developer deploy their own operating system, access the hardware, and build their applications in a self-contained environment with access to the necessary system resources.
Containers follow the same principle as virtual machines.
They provide isolated environments to run software services and optimize resources from one piece of hardware.
However, they're even more efficient.
The key difference between virtual machines and containers is that virtual machines virtualize an entire machine down to the hardware layers.
Whereas containers only virtualize software layers above the operating system level.
Containers start faster and use a fraction of the memory compared to booting an entire operating system.
A container is packaged with your application and all of its dependencies, so it has everything it needs to run.
Containers can be independently developed, tested, and deployed, and are well suited for a microservices based architecture.
This architecture is made up of smaller individual services that run containerized applications, that communicate with each other through APIs or other lightweight communication methods, such as REST or gRPC.
Containers let developers create predictable environments isolated from other system resources.
So if a customer asks for a new feature or a change in the application, developers can easily make an update to that particular part of the application without affecting the REST.
Containers can run virtually and anywhere, which makes development and deployment easy.
 
### 4.1.4 Managing containers

Containers improve agility, enhance security, optimize resources and simplify managing applications in the cloud.
Many organizations have a mix of virtual machines and containers.
However, as their It infrastructure setup becomes more complex, they often need a way to manage their services and machines.
For example, an organization can have millions and millions of containers.
This require as keeping them secure and ensuring that they operate efficiently can require significant oversight and management.
Kubernetes, originally developed by Google, is an open-source platform for managing containerized workloads and services.
It makes it easy to orchestrate many containers on many hosts, scale them, and easily deploy rollouts and rollbacks.
This improves application reliability and reduces the time and resources needed to spend on management and operations.
Google Kubernetes Engine or GKE is a Google hosted, managed Kubernetes service in the Cloud.
The GKE environment consists of multiple machines, specifically compute engine instances grouped to form a cluster.
GKE clusters can be customized, and they support different machine types, numbers of nodes, and network settings.
GKE makes it easy to deploy applications by providing an API and a Web based console.
Applications can be deployed in minutes and can be scaled up or down as needed.
GKE also provides many features that can help monitor applications, manage resources, and troubleshoot problems.
Let's explore how Ubie, a Japan based healthcare technology startup, reduced their infrastructure costs and maintenance requirements with Google Kubernetes Engine.
Founded in 2017, Ubie's goal is to get people the right medical care when they need it, and it does this with products designed for hospitals and individuals.
Ubie for hospital, their flagship product, is AI powered questionnaire software that lets patients provide medical details before an appointment.
Ubie initially relied on an alternative, Cloud, to make Ubie for Hospital available in Japan.
As the business added new customers, they needed an infrastructure that could support daily deployments and provide a secure gateway to connect Ubie to a wide range of customer networks and settings.
Ubie evaluated available options and decided to use Kubernetes in Google Kubernetes Engine.
Google Kubernetes Engine Autopilot, a mode that enables full management of an entire cluster's infrastructure and provides per-pod billing, presented a compelling option for the business to run Ubie for Hospital more efficiently and cost effectively.
With GKE Autopilot, Ubie could eliminate the need to configure and monitor clusters while only paying for running pods.
The shift reduced Ubie's infrastructure costs by 20%, and GKE Autopilot has helped the business eliminate
Ubie for Hospital infrastructure maintenance and upgrade tasks that could take hours and days to complete.
Another popular option for running containerized applications on Google Cloud is Cloud Run.
Cloud Run is a fully managed serverless platform to deploy and run containerized applications without needing to worry about the underlying infrastructure.
After your application code is containerized and deployed to Cloud Run, Google Cloud takes care of scaling and managing the infrastructure automatically.
Cloud Run is ideal for running stateless applications that need to scale up and down quickly in response to traffic.
This makes cloud run most suitable for simple and lightweight applications such as web applications.
In summary, GKE is ideal when lots of control is required over a Kubernetes Environment and there are complex applications to run.
Alternatively, Cloud Run is ideal for when a simple, fully managed serverless platform that can scale up and down quickly is required.

### 4.1.5 Serverless computing

Another option for modernizing Cloud applications is serverless computing.
Serverless computing doesn't mean there's no server, it means that resources like compute power are automatically provisioned in the background as needed.
The advantage here is that organizations won't pay for compute power unless they're running a query or application.
At its simplest definition, serverless means that businesses provide the code for whatever function they want and the public Cloud provider does everything else.
Imagine you provide software to businesses that help employees manage their corporate expenses.
You want to add a feature that lets users upload an image with their expense receipt.
In this case, the ability to upload an image is called a function.
You as the software development company write the code for that function directly into your public Cloud platform.
From there, the public Cloud provider manages everything else.
One type of serverless computing solution is called function as a service.
Some functions are a response to specific events, like file uploads to Cloud storage, or changes to database records.
You write the code that defines the response to those events and the Cloud provider does everything else.
Google Cloud offers many serverless computing products.
The first is Cloud Run, which is a fully managed environment for running containerized applications. 
With this product, you don't have to worry about the underlying infrastructure.
Then there is Cloud functions, which is the platform for hosting simple single purpose functions that are attached to events emitted from your Cloud infrastructure and services.
For example, sending a notification to a mobile device when a new order is placed on a website.
There is also App Engine, which is a service to build and deploy web applications.
Serverless computing has many benefits, reduced operational costs.
The Cloud provider is responsible for the infrastructure and its maintenance.
Therefore, the application owner does not need to invest in the infrastructure or the human resources required to manage it.
Scalability.
Serverless computing provides automatic scaling of computing resources based on the applications demand.
The Cloud provider manages the scaling process and the application owner only pays for the resources they use.
Faster time to market, the need for infrastructure, setup and configuration is eliminated, which reduces the time required to deploy applications.
This feature lets the application owner focus on writing code and quickly deploying new features.
Reduce development costs.
The development process is simplified because developers can focus on the application's logic and not on the underlying infrastructure.
Improved resilience.
Serverless computing offers improved resilience and availability as the Cloud provider automatically manages the infrastructure's failover and disaster recovery capabilities.
Pay per use pricing model, The application owner only pays for the computing resources they use.
This reduces the cost of unused resources and helps optimize costs.
How might an organization benefit from Cloud computing infrastructure technology?
Let's explore an example specializing in educational technology.
Mashme.io provides video collaboration experiences for over 3 million users in 73 countries.
Connecting 250 full HD live video streams in real time is a major technical challenge.
Latencies need to be kept very low to achieve the face to face experience, and continuous integration in deployment is vital to avoid disruptive downtime for global clients.
Meanwhile, costs have to be kept to a minimum to keep the solution affordable for a growing start up.
To meet those needs, Mashme.io chose to use Google Kubernetes Engine.
Every teacher we speak to tells us that latency is the most important thing for educational video conferencing.
Says Mashme.io meet founder Victor Sanchez Belmar.
Low latency means having servers close to every student that connects to Mashme.io.
With students connecting from around the world, Google Cloud has the global network to make that happen.
The view was that setting up data centers around the world with your own hardware is a good way for a start up to never start.
Instead, Mashme.io started using Google's global network with App Engine before moving to Google Cloud with their own docker containers, and finally, to Google Kubernetes Engine.
This allowed them to update their nodes and services in an almost continuous way without disruption.
Students didn't lose an hour or even a second of class.

## 4.2  Modernizing Applications in the Cloud

### 4.2.1 The benefits of modern cloud application development

Thanks to advances in cloud technology, the way that software applications are developed has drastically changed.
With modern cloud application development, software development is flexible, scalable, and uses the latest cloud computing technologies to build and deploy applications.
In the past, the traditional software development approach, often referred to as monolithic applications, required all the components of an application to be developed and deployed as a single, tightly coupled unit, typically using a single programming language.
There are many benefits to the modern cloud application development approach.
Let's explore a few.
We'll begin with architecture.
Modern cloud applications are typically built as a collection of microservices.
Microservices are independently deployable, scalable and maintainable components that can be used to build a wide range of applications.
This can help organizations bring business value to market faster because features can be released as they're completed without waiting for the rest of the application to be complete.
Regarding deployment, modern applications are typically deployed to the cloud and can use managed or partially managed services.
Managed services take care of the day-to-day management of cloud-based infrastructure, such as patching, upgrades, and monitoring.
This can free up staff to focus on other tasks, such as developing new applications.
Partially managed services offer a hybrid approach, where businesses manage some aspects of their cloud-based applications themselves and the cloud provider manages others.
In terms of cost, modern cloud applications use a pay as you go pricing model, which can make them extremely cost effective when configured efficiently.
That means that organizations don't always need to pay for resources they aren't fully utilizing.
Developers can also use prebuilt APIs, which we'll explore later in this section of the course, and other tools offered by the cloud provider to build and deploy their applications quicker.
And then there's scalability.
Modern cloud-based applications can easily be scaled up or down to meet user demands.
Modern cloud applications are designed to be highly available and resilient with built in features like load balancing, which is the process of distributing network traffic evenly across multiple servers that support an application.
And automatic failover, which is a process that allows a cloud-based application to automatically switch to a backup server if a failure occurs.
Additionally, cloud service providers typically offer robust monitoring and management tools that allow developers to quickly identify and respond to issues, which can further improve the reliability of cloud applications.

### 4.2.2 Rehosting legacy applications in the cloud

When a business decides to modernize  and move its operations to the cloud,  it might be running several  specialized legacy applications  that arenâ€™t compatible with  cloud-native applications.
In these situations, a business  might take a rehost migration path,  commonly referred to as lift and shift, where an application is moved from an  on-premises environment to a cloud environment without making  any changes to the application itself.
Rehosting applications brings with it   the many benefits of cloud computing  that we explored earlier, such as  cost savings, scalability,  reliability, and security.
However, there are also some potential  drawbacks to choosing a rehost migration   path for legacy applications, including: Complexity: rehosting can be a complex   process.
Businesses need to carefully  plan the migration process and ensure   that they have the right resources in place.
Risk: migrating applications to the cloud   always involves some risk.
Businesses  need to carefully assess and identify   potential risks and ensure that they have  a plan in place in case of any problems.
Vendor lock-in: by moving applications to the  cloud, businesses might become locked into a particular cloud provider.
This can potentially  make it difficult to switch providers later.
Google Cloud offers many solutions for  rehosting specialized legacy applications.
The first is Google Cloud VMware Engine, which  helps migrate existing VMware workloads to   the cloud without having to rearchitect  the applications or retool operations.
With Google Cloud VMware Engine, organizations  can maintain their existing VMware   environments and operational processes, while benefiting from the scalability,  security, reliability of Google Cloud.
By doing this, organizations can also access  a range of Google Cloud services such as  BigQuery, AI/ML, and Google Kubernetes Engine, which lets them modernize their   application environment and use new  capabilities and technologies as needed.
And for organizations with  legacy applications on Oracle,  Google Cloud offers Bare Metal Solution.
This is a fully managed cloud infrastructure  solution that lets organizations run their   Oracle workloads on dedicated,  bare metal servers in the cloud.

### 4.2.3 Application programming interfaces (APIs)

Implementing a software service can be complex and changeable.
And if each software service that an organization uses has to be coded for each implementation, the result can be fragile and error-prone.
One way to make things easier is to use APIs or application programming interfaces.
Earlier in this course, you saw how cloud providers offer a variety of resources and services for running applications and performing computational tasks in the cloud.
However, to fully use these resources and services, applications need to be able to interact with them in a standardized and efficient way.
This is where APIs come in.
An API is a set of instructions that lets different software programs communicate with each other.
Think of it as an intermediary between two different programs, which provides a standardized and predictable way for them to exchange data and interact.
An API is like a waiter in a restaurant.
The waiter takes orders from customers, communicates with the kitchen, and then brings the food back to the customers.
Similarly, an API takes requests from one software program, the customer, communicates with another program,
the kitchen, and then returns a response, the food, back to the requesting program, the customer.
APIs can be used in many different applications, from social media platforms to mobile apps and web services.
They let developers access functionality and data from other programs without having to write all the code themselves, saving time and effort.
Google itself provides many APIs that let developers access its products and services.
These include APIs that use the power of Google to search across a website or collection of websites, APIs that let developers access Google Maps data such as maps, directions and traffic information, and APIs that let developers translate text from one language to another.
In fact, many Google Cloud products and services have documented APIs.
Using APIs can create new business opportunities for organizations and improve online experiences for users.
For example, an organization could expose an API that allows customers to track their shipments or check their account balances from within a third party app.
There's also an opportunity for organizations to create new products that let other companies access their data or services through an API.
Let's explore why an organization might consider this business opportunity.
APIs can be used to create new products and services.
An organization could create an API that allows developers to access data from its database.
This data could then be used to create new products and services.
APIs can be used to generate new revenue streams.
An organization could charge developers to access its APIs.
This could generate new revenue streams for the organization and help to offset the cost of developing and maintaining the APIs.
APIs can create partnerships.
By exposing APIs, organizations can create partnerships with other companies or developers which can lead to new business opportunities and collaborations.
By carefully considering the needs of their customers and partners, organizations can develop APIs that provide value and help to grow their businesses.

### 4.2.4 Apigee API Management

When an organization has implemented API's, it's important to maintain and manage them effectively.
This can be done using a platform such as Apigee API management, Google Cloud's API management service to operate API's with enhanced scale security and automation.
Apigee is a popular choice for organizations that need to manage their API's because it offers many benefits.
It helps organizations secure their API's by providing features such as authentication, authorization and data encryption.
It tracks and analyzes API usage with real time analytics and historical reporting.
It helps with developing and deploying API's through a visual API editor and a test sandbox.
It offers API versioning, API documentation, and even API throttling, which is the process of limiting the number of API requests a user can make in a certain period.
AccuWeather has enjoyed great success, sharing its world class weather data through APIs with a range of global partners who have built applications for connected cars, smart homes, wearables, smart TV's, mobile devices, and more.
But the company wanted to get its data into the hands of a new customer, individual developers.
It needed a way to engage this audience and tailor its offerings to the varying needs of developers and monetize those different offering levels accordingly.
To implement a simple and fast way for developers to start building with an appropriate level of API calls and features for their needs, AccuWeather realized it required a sophisticated API management platform.
One that enabled different tiers of offerings by bundling API's into different products, each with their own rate limits and pricing.
With Apigee managing API's for AccuWeather, their users can customize API consumption to their specific needs.
While Apigee helps attract and build that traffic.
With the customizable Apigee developer portal, developers can sign up quickly, learn about the AccuWeather API's and test them out.
With built in analytics, AccuWeather can keep close tabs on who's signing up, what traffic they're producing and from where, and also observe unexpected patterns in traffic activity.

### 4.2.5 Hybrid and multi-cloud

As you've seen throughout this course, organizations can thrive with the help of cloud.
But the reality is that most of the world's enterprise computing still happens on premises.
The path to the cloud can be complex and full of difficult decisions and sometimes workloads remain on premises due to compliance or operational concerns.
How can organizations modernize their IT infrastructure without completely migrating to the cloud?
How can they maintain flexibility and avoid lock in?
Two options are hybrid and multi cloud solutions.
A hybrid cloud environment comprises some combination of on premises or private cloud infrastructure and public cloud services.
This is the situation many organizations are currently in, where some of their data and applications have been migrated to the cloud, while others remain on premises.
Interconnects between the private and public clouds allow interoperability.
A multi-cloud environment is where an organization uses multiple public cloud providers as part of its architecture.
This is ideal for organizations that need flexibility and secure connectivity between the different networks.
An organization might choose to use hybrid cloud multi-cloud or a combination of both if they want to incorporate specific elements of a public cloud to benefit from the main strengths of that provider.
This lets organizations keep parts of the system's infrastructure on premises while they move other parts to the cloud.
This way they create an environment that is uniquely suited to the organization's needs.
Move only specific workloads to the cloud because a full scale migration is not required for it to work, benefit from the flexibility, scalability, and lower computing costs offered by Cloud services for running specific workloads.
Add specialized services such as machine learning, content caching, data analysis, long term storage, and IOT or Internet of Things.
To the organization's computing resources toolkit.
How can Google Cloud help in this context?
Google's answer to modern hybrid and Multi-cloud distributed systems and services management is called GKE Enterprise.
GKE Enterprise is a managed production ready platform for running Kubernetes applications across multiple cloud environments.
It provides a consistent way to manage Kubernetes, clusters, applications and services regardless of where they are running.
Some of the benefits of GKE enterprise include Multi-cloud and hybrid-cloud support.
GKE enterprise can run Kubernetes clusters on Google Cloud, AWS, Azure, and other public clouds.
Centralized management GKE Enterprise provides a single centralized console for managing Kubernetes clusters and applications, security and compliance.
GKE Enterprise includes many features that help secure Kubernetes clusters and applications and comply with industry regulations, networking and load balancing.
GKE Enterprise includes a number of features that help network and load balance Kubernetes applications, monitoring and logging GKE Enterprise provides a rich set of tools for monitoring and maintaining application consistency across an entire network, whether on premises in the cloud or in multiple clouds.

## 4.3 Course Summary

### 4.3.1 Summary

This brings us to the end of the modernized infrastructure and applications with Google Cloud course.
Let's do a quick recap.
In the first section of the course titled course introduction, you explored some important cloud migration terminology.
In the second section, titled modernizing infrastructure in the cloud, you were introduced to, the benefits of running compute workloads in the cloud.
Virtual machines, containers and how to manage them, and serverless computing.
And in the final section of the course, modernizing applications in the cloud, you learned about the benefits of modern cloud application development.
Rehosting legacy applications in the cloud, APIs and API management with Apigee, and using hybrid and multi-cloud solutions.
Now that you have a comprehensive introduction to modernizing infrastructure and applications on Google Cloud, you can move on to the next course in the series.
Trust and Security with Google Cloud, where you'll learn about fundamental cloud security concepts, google's multi layered approach to infrastructure security, and how Google Cloud strives to earn and maintain customer trust in the cloud.
We'll see you next time.
